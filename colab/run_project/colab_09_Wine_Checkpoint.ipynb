{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"colab_09_Wine_Checkpoint.ipynb","provenance":[{"file_id":"1N7b6A2FrMgd84BAKWgoHpbuhkc9ahu4h","timestamp":1585420853543}]}},"cells":[{"cell_type":"code","metadata":{"id":"tDufv2Hbb4xf","colab_type":"code","outputId":"c6801b05-31fa-4a0d-da96-1c8fc50018c8","executionInfo":{"status":"ok","timestamp":1585421541610,"user_tz":240,"elapsed":37440,"user":{"displayName":"Taeho Jo","photoUrl":"https://lh6.googleusercontent.com/-R0w90jUICh4/AAAAAAAAAAI/AAAAAAAABi8/Vvpvj-kW8KA/s64/photo.jpg","userId":"10835425045255184608"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":1000}},"source":["# 데이터 입력\n","from google.colab import files\n","uploaded = files.upload()\n","my_data = 'wine.csv'\n","\n","!pip install -q tensorflow-gpu==1.15.0\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import pandas as pd\n","import numpy\n","import os\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint\n","\n","# seed 값 설정\n","numpy.random.seed(3)\n","tf.compat.v1.set_random_seed(3)\n","\n","# 데이터 적용\n","df_pre = pd.read_csv(my_data, header=None)\n","df = df_pre.sample(frac=1)\n","\n","dataset = df.values\n","X = dataset[:,0:12]\n","Y = dataset[:,12]\n","\n","# 모델의 설정\n","model = Sequential()\n","model.add(Dense(30,  input_dim=12, activation='relu'))\n","model.add(Dense(12, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# 모델 컴파일\n","model.compile(loss='binary_crossentropy',\n","          optimizer='adam',\n","          metrics=['accuracy'])\n","\n","# 모델 저장 폴더 설정\n","MODEL_DIR = './model/'\n","if not os.path.exists(MODEL_DIR):\n","   os.mkdir(MODEL_DIR)\n","\n","# 모델 저장 조건 설정\n","modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n","\n","# 모델 실행 및 저장\n","model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])\n"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-1169aee5-9906-4f06-99d5-67a7546c43cc\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-1169aee5-9906-4f06-99d5-67a7546c43cc\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving wine.csv to wine.csv\n","\u001b[K     |███████████████████████▏        | 297.4MB 64.3MB/s eta 0:00:02\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n","\u001b[?25hTensorFlow 1.x selected.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","\n","Epoch 00001: val_loss improved from inf to 0.35957, saving model to ./model/01-0.3596.hdf5\n","\n","Epoch 00002: val_loss improved from 0.35957 to 0.31446, saving model to ./model/02-0.3145.hdf5\n","\n","Epoch 00003: val_loss improved from 0.31446 to 0.29033, saving model to ./model/03-0.2903.hdf5\n","\n","Epoch 00004: val_loss improved from 0.29033 to 0.27195, saving model to ./model/04-0.2720.hdf5\n","\n","Epoch 00005: val_loss improved from 0.27195 to 0.24896, saving model to ./model/05-0.2490.hdf5\n","\n","Epoch 00006: val_loss improved from 0.24896 to 0.23058, saving model to ./model/06-0.2306.hdf5\n","\n","Epoch 00007: val_loss improved from 0.23058 to 0.21621, saving model to ./model/07-0.2162.hdf5\n","\n","Epoch 00008: val_loss improved from 0.21621 to 0.20368, saving model to ./model/08-0.2037.hdf5\n","\n","Epoch 00009: val_loss improved from 0.20368 to 0.19401, saving model to ./model/09-0.1940.hdf5\n","\n","Epoch 00010: val_loss improved from 0.19401 to 0.18665, saving model to ./model/10-0.1866.hdf5\n","\n","Epoch 00011: val_loss improved from 0.18665 to 0.18263, saving model to ./model/11-0.1826.hdf5\n","\n","Epoch 00012: val_loss did not improve from 0.18263\n","\n","Epoch 00013: val_loss improved from 0.18263 to 0.17544, saving model to ./model/13-0.1754.hdf5\n","\n","Epoch 00014: val_loss improved from 0.17544 to 0.17061, saving model to ./model/14-0.1706.hdf5\n","\n","Epoch 00015: val_loss improved from 0.17061 to 0.16335, saving model to ./model/15-0.1634.hdf5\n","\n","Epoch 00016: val_loss improved from 0.16335 to 0.16002, saving model to ./model/16-0.1600.hdf5\n","\n","Epoch 00017: val_loss improved from 0.16002 to 0.15613, saving model to ./model/17-0.1561.hdf5\n","\n","Epoch 00018: val_loss improved from 0.15613 to 0.15166, saving model to ./model/18-0.1517.hdf5\n","\n","Epoch 00019: val_loss improved from 0.15166 to 0.14855, saving model to ./model/19-0.1485.hdf5\n","\n","Epoch 00020: val_loss improved from 0.14855 to 0.14603, saving model to ./model/20-0.1460.hdf5\n","\n","Epoch 00021: val_loss improved from 0.14603 to 0.14444, saving model to ./model/21-0.1444.hdf5\n","\n","Epoch 00022: val_loss improved from 0.14444 to 0.13780, saving model to ./model/22-0.1378.hdf5\n","\n","Epoch 00023: val_loss improved from 0.13780 to 0.13433, saving model to ./model/23-0.1343.hdf5\n","\n","Epoch 00024: val_loss improved from 0.13433 to 0.13057, saving model to ./model/24-0.1306.hdf5\n","\n","Epoch 00025: val_loss did not improve from 0.13057\n","\n","Epoch 00026: val_loss improved from 0.13057 to 0.12683, saving model to ./model/26-0.1268.hdf5\n","\n","Epoch 00027: val_loss improved from 0.12683 to 0.12036, saving model to ./model/27-0.1204.hdf5\n","\n","Epoch 00028: val_loss improved from 0.12036 to 0.11638, saving model to ./model/28-0.1164.hdf5\n","\n","Epoch 00029: val_loss improved from 0.11638 to 0.11298, saving model to ./model/29-0.1130.hdf5\n","\n","Epoch 00030: val_loss improved from 0.11298 to 0.11256, saving model to ./model/30-0.1126.hdf5\n","\n","Epoch 00031: val_loss improved from 0.11256 to 0.10774, saving model to ./model/31-0.1077.hdf5\n","\n","Epoch 00032: val_loss did not improve from 0.10774\n","\n","Epoch 00033: val_loss improved from 0.10774 to 0.10378, saving model to ./model/33-0.1038.hdf5\n","\n","Epoch 00034: val_loss improved from 0.10378 to 0.10230, saving model to ./model/34-0.1023.hdf5\n","\n","Epoch 00035: val_loss improved from 0.10230 to 0.10037, saving model to ./model/35-0.1004.hdf5\n","\n","Epoch 00036: val_loss improved from 0.10037 to 0.09989, saving model to ./model/36-0.0999.hdf5\n","\n","Epoch 00037: val_loss improved from 0.09989 to 0.09734, saving model to ./model/37-0.0973.hdf5\n","\n","Epoch 00038: val_loss improved from 0.09734 to 0.09439, saving model to ./model/38-0.0944.hdf5\n","\n","Epoch 00039: val_loss did not improve from 0.09439\n","\n","Epoch 00040: val_loss did not improve from 0.09439\n","\n","Epoch 00041: val_loss improved from 0.09439 to 0.09092, saving model to ./model/41-0.0909.hdf5\n","\n","Epoch 00042: val_loss did not improve from 0.09092\n","\n","Epoch 00043: val_loss did not improve from 0.09092\n","\n","Epoch 00044: val_loss improved from 0.09092 to 0.09024, saving model to ./model/44-0.0902.hdf5\n","\n","Epoch 00045: val_loss improved from 0.09024 to 0.08989, saving model to ./model/45-0.0899.hdf5\n","\n","Epoch 00046: val_loss improved from 0.08989 to 0.08825, saving model to ./model/46-0.0882.hdf5\n","\n","Epoch 00047: val_loss improved from 0.08825 to 0.08422, saving model to ./model/47-0.0842.hdf5\n","\n","Epoch 00048: val_loss improved from 0.08422 to 0.08346, saving model to ./model/48-0.0835.hdf5\n","\n","Epoch 00049: val_loss did not improve from 0.08346\n","\n","Epoch 00050: val_loss did not improve from 0.08346\n","\n","Epoch 00051: val_loss did not improve from 0.08346\n","\n","Epoch 00052: val_loss did not improve from 0.08346\n","\n","Epoch 00053: val_loss did not improve from 0.08346\n","\n","Epoch 00054: val_loss did not improve from 0.08346\n","\n","Epoch 00055: val_loss did not improve from 0.08346\n","\n","Epoch 00056: val_loss improved from 0.08346 to 0.08018, saving model to ./model/56-0.0802.hdf5\n","\n","Epoch 00057: val_loss improved from 0.08018 to 0.07726, saving model to ./model/57-0.0773.hdf5\n","\n","Epoch 00058: val_loss improved from 0.07726 to 0.07603, saving model to ./model/58-0.0760.hdf5\n","\n","Epoch 00059: val_loss did not improve from 0.07603\n","\n","Epoch 00060: val_loss did not improve from 0.07603\n","\n","Epoch 00061: val_loss improved from 0.07603 to 0.07423, saving model to ./model/61-0.0742.hdf5\n","\n","Epoch 00062: val_loss did not improve from 0.07423\n","\n","Epoch 00063: val_loss did not improve from 0.07423\n","\n","Epoch 00064: val_loss improved from 0.07423 to 0.07202, saving model to ./model/64-0.0720.hdf5\n","\n","Epoch 00065: val_loss improved from 0.07202 to 0.07163, saving model to ./model/65-0.0716.hdf5\n","\n","Epoch 00066: val_loss improved from 0.07163 to 0.07086, saving model to ./model/66-0.0709.hdf5\n","\n","Epoch 00067: val_loss did not improve from 0.07086\n","\n","Epoch 00068: val_loss improved from 0.07086 to 0.07042, saving model to ./model/68-0.0704.hdf5\n","\n","Epoch 00069: val_loss did not improve from 0.07042\n","\n","Epoch 00070: val_loss improved from 0.07042 to 0.06971, saving model to ./model/70-0.0697.hdf5\n","\n","Epoch 00071: val_loss improved from 0.06971 to 0.06969, saving model to ./model/71-0.0697.hdf5\n","\n","Epoch 00072: val_loss did not improve from 0.06969\n","\n","Epoch 00073: val_loss improved from 0.06969 to 0.06806, saving model to ./model/73-0.0681.hdf5\n","\n","Epoch 00074: val_loss did not improve from 0.06806\n","\n","Epoch 00075: val_loss did not improve from 0.06806\n","\n","Epoch 00076: val_loss did not improve from 0.06806\n","\n","Epoch 00077: val_loss improved from 0.06806 to 0.06724, saving model to ./model/77-0.0672.hdf5\n","\n","Epoch 00078: val_loss improved from 0.06724 to 0.06636, saving model to ./model/78-0.0664.hdf5\n","\n","Epoch 00079: val_loss did not improve from 0.06636\n","\n","Epoch 00080: val_loss did not improve from 0.06636\n","\n","Epoch 00081: val_loss did not improve from 0.06636\n","\n","Epoch 00082: val_loss did not improve from 0.06636\n","\n","Epoch 00083: val_loss improved from 0.06636 to 0.06445, saving model to ./model/83-0.0645.hdf5\n","\n","Epoch 00084: val_loss did not improve from 0.06445\n","\n","Epoch 00085: val_loss did not improve from 0.06445\n","\n","Epoch 00086: val_loss improved from 0.06445 to 0.06284, saving model to ./model/86-0.0628.hdf5\n","\n","Epoch 00087: val_loss did not improve from 0.06284\n","\n","Epoch 00088: val_loss did not improve from 0.06284\n","\n","Epoch 00089: val_loss did not improve from 0.06284\n","\n","Epoch 00090: val_loss did not improve from 0.06284\n","\n","Epoch 00091: val_loss did not improve from 0.06284\n","\n","Epoch 00092: val_loss did not improve from 0.06284\n","\n","Epoch 00093: val_loss did not improve from 0.06284\n","\n","Epoch 00094: val_loss did not improve from 0.06284\n","\n","Epoch 00095: val_loss improved from 0.06284 to 0.06010, saving model to ./model/95-0.0601.hdf5\n","\n","Epoch 00096: val_loss did not improve from 0.06010\n","\n","Epoch 00097: val_loss did not improve from 0.06010\n","\n","Epoch 00098: val_loss did not improve from 0.06010\n","\n","Epoch 00099: val_loss did not improve from 0.06010\n","\n","Epoch 00100: val_loss improved from 0.06010 to 0.05935, saving model to ./model/100-0.0593.hdf5\n","\n","Epoch 00101: val_loss did not improve from 0.05935\n","\n","Epoch 00102: val_loss improved from 0.05935 to 0.05822, saving model to ./model/102-0.0582.hdf5\n","\n","Epoch 00103: val_loss improved from 0.05822 to 0.05811, saving model to ./model/103-0.0581.hdf5\n","\n","Epoch 00104: val_loss did not improve from 0.05811\n","\n","Epoch 00105: val_loss did not improve from 0.05811\n","\n","Epoch 00106: val_loss did not improve from 0.05811\n","\n","Epoch 00107: val_loss did not improve from 0.05811\n","\n","Epoch 00108: val_loss improved from 0.05811 to 0.05801, saving model to ./model/108-0.0580.hdf5\n","\n","Epoch 00109: val_loss did not improve from 0.05801\n","\n","Epoch 00110: val_loss did not improve from 0.05801\n","\n","Epoch 00111: val_loss did not improve from 0.05801\n","\n","Epoch 00112: val_loss did not improve from 0.05801\n","\n","Epoch 00113: val_loss did not improve from 0.05801\n","\n","Epoch 00114: val_loss improved from 0.05801 to 0.05760, saving model to ./model/114-0.0576.hdf5\n","\n","Epoch 00115: val_loss did not improve from 0.05760\n","\n","Epoch 00116: val_loss did not improve from 0.05760\n","\n","Epoch 00117: val_loss did not improve from 0.05760\n","\n","Epoch 00118: val_loss improved from 0.05760 to 0.05654, saving model to ./model/118-0.0565.hdf5\n","\n","Epoch 00119: val_loss did not improve from 0.05654\n","\n","Epoch 00120: val_loss improved from 0.05654 to 0.05573, saving model to ./model/120-0.0557.hdf5\n","\n","Epoch 00121: val_loss did not improve from 0.05573\n","\n","Epoch 00122: val_loss did not improve from 0.05573\n","\n","Epoch 00123: val_loss improved from 0.05573 to 0.05554, saving model to ./model/123-0.0555.hdf5\n","\n","Epoch 00124: val_loss did not improve from 0.05554\n","\n","Epoch 00125: val_loss improved from 0.05554 to 0.05532, saving model to ./model/125-0.0553.hdf5\n","\n","Epoch 00126: val_loss did not improve from 0.05532\n","\n","Epoch 00127: val_loss did not improve from 0.05532\n","\n","Epoch 00128: val_loss did not improve from 0.05532\n","\n","Epoch 00129: val_loss improved from 0.05532 to 0.05473, saving model to ./model/129-0.0547.hdf5\n","\n","Epoch 00130: val_loss did not improve from 0.05473\n","\n","Epoch 00131: val_loss did not improve from 0.05473\n","\n","Epoch 00132: val_loss did not improve from 0.05473\n","\n","Epoch 00133: val_loss did not improve from 0.05473\n","\n","Epoch 00134: val_loss did not improve from 0.05473\n","\n","Epoch 00135: val_loss did not improve from 0.05473\n","\n","Epoch 00136: val_loss did not improve from 0.05473\n","\n","Epoch 00137: val_loss improved from 0.05473 to 0.05353, saving model to ./model/137-0.0535.hdf5\n","\n","Epoch 00138: val_loss did not improve from 0.05353\n","\n","Epoch 00139: val_loss did not improve from 0.05353\n","\n","Epoch 00140: val_loss did not improve from 0.05353\n","\n","Epoch 00141: val_loss did not improve from 0.05353\n","\n","Epoch 00142: val_loss did not improve from 0.05353\n","\n","Epoch 00143: val_loss improved from 0.05353 to 0.05321, saving model to ./model/143-0.0532.hdf5\n","\n","Epoch 00144: val_loss did not improve from 0.05321\n","\n","Epoch 00145: val_loss did not improve from 0.05321\n","\n","Epoch 00146: val_loss did not improve from 0.05321\n","\n","Epoch 00147: val_loss did not improve from 0.05321\n","\n","Epoch 00148: val_loss improved from 0.05321 to 0.05273, saving model to ./model/148-0.0527.hdf5\n","\n","Epoch 00149: val_loss did not improve from 0.05273\n","\n","Epoch 00150: val_loss did not improve from 0.05273\n","\n","Epoch 00151: val_loss did not improve from 0.05273\n","\n","Epoch 00152: val_loss improved from 0.05273 to 0.05240, saving model to ./model/152-0.0524.hdf5\n","\n","Epoch 00153: val_loss did not improve from 0.05240\n","\n","Epoch 00154: val_loss did not improve from 0.05240\n","\n","Epoch 00155: val_loss did not improve from 0.05240\n","\n","Epoch 00156: val_loss did not improve from 0.05240\n","\n","Epoch 00157: val_loss did not improve from 0.05240\n","\n","Epoch 00158: val_loss did not improve from 0.05240\n","\n","Epoch 00159: val_loss did not improve from 0.05240\n","\n","Epoch 00160: val_loss improved from 0.05240 to 0.05152, saving model to ./model/160-0.0515.hdf5\n","\n","Epoch 00161: val_loss improved from 0.05152 to 0.05128, saving model to ./model/161-0.0513.hdf5\n","\n","Epoch 00162: val_loss did not improve from 0.05128\n","\n","Epoch 00163: val_loss did not improve from 0.05128\n","\n","Epoch 00164: val_loss did not improve from 0.05128\n","\n","Epoch 00165: val_loss did not improve from 0.05128\n","\n","Epoch 00166: val_loss did not improve from 0.05128\n","\n","Epoch 00167: val_loss did not improve from 0.05128\n","\n","Epoch 00168: val_loss did not improve from 0.05128\n","\n","Epoch 00169: val_loss improved from 0.05128 to 0.05089, saving model to ./model/169-0.0509.hdf5\n","\n","Epoch 00170: val_loss did not improve from 0.05089\n","\n","Epoch 00171: val_loss improved from 0.05089 to 0.05035, saving model to ./model/171-0.0503.hdf5\n","\n","Epoch 00172: val_loss did not improve from 0.05035\n","\n","Epoch 00173: val_loss did not improve from 0.05035\n","\n","Epoch 00174: val_loss did not improve from 0.05035\n","\n","Epoch 00175: val_loss did not improve from 0.05035\n","\n","Epoch 00176: val_loss did not improve from 0.05035\n","\n","Epoch 00177: val_loss did not improve from 0.05035\n","\n","Epoch 00178: val_loss did not improve from 0.05035\n","\n","Epoch 00179: val_loss did not improve from 0.05035\n","\n","Epoch 00180: val_loss did not improve from 0.05035\n","\n","Epoch 00181: val_loss improved from 0.05035 to 0.05025, saving model to ./model/181-0.0502.hdf5\n","\n","Epoch 00182: val_loss did not improve from 0.05025\n","\n","Epoch 00183: val_loss did not improve from 0.05025\n","\n","Epoch 00184: val_loss did not improve from 0.05025\n","\n","Epoch 00185: val_loss improved from 0.05025 to 0.05009, saving model to ./model/185-0.0501.hdf5\n","\n","Epoch 00186: val_loss improved from 0.05009 to 0.04995, saving model to ./model/186-0.0500.hdf5\n","\n","Epoch 00187: val_loss did not improve from 0.04995\n","\n","Epoch 00188: val_loss did not improve from 0.04995\n","\n","Epoch 00189: val_loss did not improve from 0.04995\n","\n","Epoch 00190: val_loss improved from 0.04995 to 0.04978, saving model to ./model/190-0.0498.hdf5\n","\n","Epoch 00191: val_loss improved from 0.04978 to 0.04911, saving model to ./model/191-0.0491.hdf5\n","\n","Epoch 00192: val_loss did not improve from 0.04911\n","\n","Epoch 00193: val_loss did not improve from 0.04911\n","\n","Epoch 00194: val_loss did not improve from 0.04911\n","\n","Epoch 00195: val_loss did not improve from 0.04911\n","\n","Epoch 00196: val_loss did not improve from 0.04911\n","\n","Epoch 00197: val_loss improved from 0.04911 to 0.04910, saving model to ./model/197-0.0491.hdf5\n","\n","Epoch 00198: val_loss did not improve from 0.04910\n","\n","Epoch 00199: val_loss did not improve from 0.04910\n","\n","Epoch 00200: val_loss did not improve from 0.04910\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7ff1b4515a58>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"aHOE5dawb4xl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}